{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuwbNTy9qmrf"
      },
      "outputs": [],
      "source": [
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TorjzPV_rbdY"
      },
      "outputs": [],
      "source": [
        "import bs4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZNaHVcNrnS3"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "id": "P-BVLXCisVEw",
        "outputId": "61f03258-1511-4c7d-beca-5a87b7cd0991"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nurls = [\\n    'https://en.wikipedia.org/wiki/Web_scraping',\\n    'https://en.wikipedia.org/wiki/Machine_learning',\\n    'https://en.wikipedia.org/wiki/Artificial_intelligence',\\n    'https://en.wikipedia.org/wiki/Gradient_descent'\\n]\\nwith open('Wikipedia_scarp.txt','w') as f:\\n  for url in urls:\\n  # Send a GET request to the URL and retrieve the HTML content\\n    response = requests.get(url)\\n    html_content = response.content\\n\\n    # Parse the HTML content using Beautiful Soup\\n    soup = BeautifulSoup(html_content, 'html.parser')\\n\\n    # Find the main content of the page (typically inside a div or section tag)\\n    content_div = soup.find('div', {'id': 'mw-content-text'})\\n\\n    # Extract the text from the content div\\n    text = content_div.get_text()\\n    \\n    # Print the scraped text\\n    f.write(text)\\n    \\n\\nprint('Scraped text has been written to file')\\n\""
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "urls = [\n",
        "    'https://en.wikipedia.org/wiki/Web_scraping',\n",
        "    'https://en.wikipedia.org/wiki/Machine_learning',\n",
        "    'https://en.wikipedia.org/wiki/Artificial_intelligence',\n",
        "    'https://en.wikipedia.org/wiki/Gradient_descent'\n",
        "]\n",
        "with open('Wikipedia_scarp.txt','w') as f:\n",
        "  for url in urls:\n",
        "  # Send a GET request to the URL and retrieve the HTML content\n",
        "    response = requests.get(url)\n",
        "    html_content = response.content\n",
        "\n",
        "    # Parse the HTML content using Beautiful Soup\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "    # Find the main content of the page (typically inside a div or section tag)\n",
        "    content_div = soup.find('div', {'id': 'mw-content-text'})\n",
        "\n",
        "    # Extract the text from the content div\n",
        "    text = content_div.get_text()\n",
        "\n",
        "    # Print the scraped text\n",
        "    f.write(text)\n",
        "\n",
        "\n",
        "print('Scraped text has been written to file')\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYxyE725-GuL",
        "outputId": "ff0f3686-89ec-4195-da20-8e78fad2d261"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wJnRAIz58pJ"
      },
      "source": [
        "# This is the method where we manuallly write the names in the list\n",
        "# Very much Boring.....\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-V0FvwX8C_hd"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "base_url = \"https://en.wikipedia.org/wiki/\"\n",
        "\n",
        "articles = [\"Mahatma Gandhi\",\"Data_science\", \"Python_(programming_language)\",\"Machine_learning\",\"Artificiall_intelligence\"]\n",
        "\n",
        "df = pd.DataFrame(columns=[\"Title\", \"Paragraphs\"])\n",
        "\n",
        "for article in articles[0:1]:\n",
        "  url = base_url + article\n",
        "  response = requests.get(url)\n",
        "  html = response.content\n",
        "\n",
        "  soup = BeautifulSoup(html,'html.parser')\n",
        "\n",
        "  title = soup.find('h1',{'class':'firstHeading'}).text\n",
        "\n",
        "  summary_div = soup.find('div', {'class': 'toc'})\n",
        "  if summary_div is not None:\n",
        "    summary = summary_div.text\n",
        "  else:\n",
        "    summary = ''\n",
        "\n",
        "\n",
        "  text = soup.find('div',{'class':'mw-parser-output'}).text\n",
        "  text_div = soup.find('div', {'class': 'mw-parser-output'})\n",
        "  if text_div is not None:\n",
        "    text = text_div.text\n",
        "  else:\n",
        "    text = ''\n",
        "\n",
        "  paragraphs = []\n",
        "  text_div = soup.find('div', {'class': 'mw-parser-output'})\n",
        "  if text_div is not None:\n",
        "    for p in text_div.find_all('p'):\n",
        "      paragraphs.append(p.text)\n",
        "\n",
        "\n",
        "  df.loc[len(df)] = [title, \"\\n\".join(paragraphs)]\n",
        "\n",
        "df.to_csv(\"wiki.csv\",index = False)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twc7CI0op22z"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "base_url = \"https://en.wikipedia.org/wiki/\"\n",
        "\n",
        "articles = [\"Mahatma Gandhi\",\"Data_science\", \"Python_(programming_language)\",\"Machine_learning\",\"Artificiall_intelligence\"]\n",
        "\n",
        "df = pd.DataFrame(columns=[\"Title\", \"Paragraphs\"])\n",
        "\n",
        "for article in articles:\n",
        "  url = base_url + article\n",
        "  response = requests.get(url)\n",
        "  html = response.content\n",
        "\n",
        "  soup = BeautifulSoup(html,'html.parser')\n",
        "\n",
        "  title = soup.find('h1',{'class':'firstHeading'}).text\n",
        "\n",
        "  paragraphs = []\n",
        "  text_div = soup.find('div', {'class': 'mw-content-container'})\n",
        "  if text_div is not None:\n",
        "    for p in text_div.find_all('p'):\n",
        "      paragraphs.append(p.text)\n",
        "\n",
        "  df.loc[len(df)] = [title, \"\\n\".join(paragraphs)]\n",
        "\n",
        "df.to_csv(\"wiki.csv\",index = False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 16
        },
        "id": "Fw7UAFkTU5X6",
        "outputId": "189170de-8c95-4f4f-f1c9-9203ba81303d"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_c959ef55-afdd-42ab-a5df-162f2290e450\", \"wikipedia_data.csv\", 27)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "df.to_csv(\"wikipedia_data.csv\", index=False)\n",
        "\n",
        "files.download(\"wikipedia_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rV2aVSeW-wc",
        "outputId": "b1a949bc-7891-468c-ca20-eecb2d379835"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "csv_path = \"/content/drive/MyDrive/wikipedia_data.csv\"\n",
        "\n",
        "df.to_csv(csv_path, index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnXFBF4oujGf"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "# Parameters for the API request\n",
        "category = input(\"Enter the Wikipedia category: \")\n",
        "url = \"https://en.wikipedia.org/w/api.php\"\n",
        "params = {\n",
        "    \"action\": \"query\",\n",
        "    \"format\": \"json\",\n",
        "    \"list\": \"categorymembers\",\n",
        "    \"cmtitle\": category,\n",
        "    \"cmlimit\": \"max\"\n",
        "}\n",
        "\n",
        "# Send API request and parse response\n",
        "response = requests.get(url, params=params)\n",
        "data = json.loads(response.text)\n",
        "\n",
        "# Check if the response has a \"query\" key\n",
        "if \"query\" in data:\n",
        "    # Extract article titles from response\n",
        "    articles = [article[\"title\"] for article in data[\"query\"][\"categorymembers\"]]\n",
        "\n",
        "    # Save articles to file\n",
        "    with open(\"articles.txt\", \"w\") as f:\n",
        "        for article in articles:\n",
        "            f.write(article + \"\\n\")\n",
        "\n",
        "    print(\"List of articles saved to articles.txt\")\n",
        "else:\n",
        "    print(\"No articles found in the specified category.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9yQo6bt0goT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Save articles to file\n",
        "file_path = os.path.join(os.getcwd(), \"articles.txt\")\n",
        "with open(\"/content/drive/MyDrive/wikipedia_data.csv\", \"a\") as f:\n",
        "    for article in articles:\n",
        "        f.write(article + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmWXOPlr6Sxj"
      },
      "source": [
        "# But this method is pretty much good...."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xwUWAG6XIi2",
        "outputId": "00fdf893-8136-41d7-9417-7fb14d7342cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "with open('/content/drive/MyDrive/articles.txt', 'r') as f:\n",
        "    articles = [line.strip() for line in f.readlines()]\n",
        "\n",
        "base_url = \"https://en.wikipedia.org/wiki/\"\n",
        "\n",
        "df = pd.DataFrame(columns=[\"Title\", \"Paragraphs\"])\n",
        "\n",
        "for article in articles:\n",
        "  url = base_url + article\n",
        "  response = requests.get(url)\n",
        "  html = response.content\n",
        "\n",
        "  soup = BeautifulSoup(html,'html.parser')\n",
        "\n",
        "  title = soup.find('h1',{'class':'firstHeading'}).text\n",
        "\n",
        "\n",
        "  paragraphs = []\n",
        "  text_div = soup.find('div', {'class': 'mw-content-container'})\n",
        "  if text_div is not None:\n",
        "    for p in text_div.find_all('p'):\n",
        "      paragraphs.append(p.text)\n",
        "\n",
        "  df.loc[len(df)] = [title, \"\\n\".join(paragraphs)]\n",
        "\n",
        "  if response.status_code != 200:\n",
        "    print(f\"The article '{article}' does not exists on Wikipedia.\")\n",
        "    articles.remove(article)\n",
        "  #else:\n",
        "    #print(f\"The article '{article}' does not exist on Wikipedia.\")\n",
        "\n",
        "with open('/content/drive/MyDrive/articles.txt', 'w') as f:\n",
        "    f.write('\\n'.join(articles))\n",
        "\n",
        "df.to_csv('/content/drive/MyDrive/wikipedia1.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pH6IjjKG8yKh"
      },
      "source": [
        "# Working on this....to make more efficientðŸ™ƒðŸ™ƒðŸ™ƒ\n",
        "All done...But don't use this code as it will not save the results directly in folder....\n",
        "It is just like i used for refernce..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1J9-M8j2pMbs",
        "outputId": "9ff7cfc1-0da2-453c-9a9e-1225ccc7f93b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Enter the Wikipedia category: Category:Chemistry\n",
            "List of articles saved to articles.txt\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "category = input(\"Enter the Wikipedia category: \")\n",
        "url = \"https://en.wikipedia.org/w/api.php\"\n",
        "params = {\n",
        "    \"action\": \"query\",\n",
        "    \"format\": \"json\",\n",
        "    \"list\": \"categorymembers\",\n",
        "    \"cmtitle\": category,\n",
        "    \"cmlimit\": \"max\"\n",
        "}\n",
        "\n",
        "response = requests.get(url, params=params)\n",
        "data = json.loads(response.text)\n",
        "\n",
        "if \"query\" in data:\n",
        "    articles = [article[\"title\"] for article in data[\"query\"][\"categorymembers\"]]\n",
        "\n",
        "    with open(\"articles.txt\", \"w\") as f:\n",
        "        for article in articles:\n",
        "            f.write(article + \"\\n\")\n",
        "\n",
        "    print(\"List of articles saved to articles.txt\")\n",
        "else:\n",
        "    print(\"No articles found in the specified category.\")\n",
        "\n",
        "with open('/content/drive/MyDrive/Wiki-datasets/articles.txt', 'r') as f:\n",
        "    articles = [line.strip() for line in f.readlines()]\n",
        "\n",
        "base_url = \"https://en.wikipedia.org/wiki/\"\n",
        "\n",
        "df = pd.DataFrame(columns=[ \"Title\", \"Paragraphs\"])\n",
        "\n",
        "for article in articles:\n",
        "  url = base_url + article\n",
        "  response = requests.get(url)\n",
        "  html = response.content\n",
        "\n",
        "  soup = BeautifulSoup(html,'html.parser')\n",
        "\n",
        "  title = soup.find('h1',{'class':'firstHeading'}).text\n",
        "\n",
        "  paragraphs = []\n",
        "  text_div = soup.find('div', {'class': 'mw-content-container'})\n",
        "  if text_div is not None:\n",
        "    for p in text_div.find_all('p'):\n",
        "      paragraphs.append(p.text)\n",
        "\n",
        "  df.loc[len(df)] = [title, \"\\n\".join(paragraphs)]\n",
        "\n",
        "  if response.status_code != 200:\n",
        "    print(f\"The article '{article}' does not exist on Wikipedia.\")\n",
        "    df = df[df[\"Article\"] != article]\n",
        "\n",
        "df.to_csv('/content/drive/MyDrive/Wiki-datasets/wikipedia_data.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OoBWH7_37Efn"
      },
      "outputs": [],
      "source": [
        "!cp /content/drive/MyDrive/articles.txt /content/drive/MyDrive/Wiki-datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xE9KJSBAysh"
      },
      "outputs": [],
      "source": [
        "!cp /content/drive/MyDrive/wikipedia_data.csv /content/drive/MyDrive/Wiki-datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIQFXEZuA4qv"
      },
      "outputs": [],
      "source": [
        "!cp /content/autoarticles.txt /content/drive/MyDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBBaiZQqOgqF"
      },
      "outputs": [],
      "source": [
        "!cp /content/autoarticles.txt /content/drive/MyDrive/autoarticles.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CEYDyrZ9p2Zv"
      },
      "outputs": [],
      "source": [
        "!cp /content/drive/MyDrive/ColabNotebooks/WikipediaScraping.ipynb /content/drive/MyDrive/Wiki-datasets/WikipediaScraping.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNfbtugLsEPc"
      },
      "source": [
        "# THIS IS WHAT U NEED.....Finallly. âœ…\n",
        "To take input use (Category:\"SUBJECT U WANNA TAKE AS INPUT\") and hit enter.\n",
        "#EXPLAIN\n",
        "in the below code it will directly store the name of articles of a particular subject without scraping the previous articles we had already scrapped....so the less chance to make Wikipedia 's server loaded with many requests...\n",
        "#Limitations\n",
        "it will not scrape the subject which have more than 5000 articles .i.e 5000 pages.\n",
        "Aditionally, it's important to be mindful of Wikipedia's API usage limits and not to make too many requests too quickly, as this can result in your IP address being blocked.\n",
        "#How to deal???\n",
        " example\n",
        "####ðŸ‘‰ðŸ»United States: As of April 2023, the English-language Wikipedia has over 6.4 million articles, with the United States being one of the most extensively covered subjects.\n",
        "####ðŸ‘‰ðŸ»Science: The field of science covers a broad range of subjects, including physics, chemistry, biology, astronomy, and more. Each of these subjects has a large number of articles and pages dedicated to them.\n",
        "####ðŸ‘‰ðŸ»History: The subject of history covers a vast array of topics, with a large number of articles and pages dedicated to specific events, periods, and people throughout history.\n",
        "\n",
        "####So now the question how to scrape it subject called Sciene here. Here's one way is to make a particualr subject an umbrella term for all the other subjects falling into that category. So here take astronomy as one subject and start scraping it and u will get desired results...ðŸ™ƒ\n",
        "\n",
        "####Another approach is to use a more powerful server or distributed computing to handle the scraping.But this approch i feel in itself is a limitation so...."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6j2KIs3D4IAz",
        "outputId": "b4bc099b-689d-4751-eab0-f118e649cf22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUZXJjzBl0EL",
        "outputId": "1792a4a5-3c74-4937-8042-400073fed3c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Enter the Wikipedia category: Category:Stage theories\n",
            "60 new articles found and saved to society.txt\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "path = '/content/drive/MyDrive/Wiki-datasets/'\n",
        "\n",
        "if os.path.isfile(path + 'society.txt'):\n",
        "\n",
        "  with open(path + 'society.txt', 'r') as f:\n",
        "    articles = [line.strip() for line in f.readlines()]\n",
        "else:\n",
        "\n",
        "  articles = []\n",
        "\n",
        "category = input(\"Enter the Wikipedia category: \")\n",
        "url = \"https://en.wikipedia.org/w/api.php\"\n",
        "params = {\n",
        "    \"action\": \"query\",\n",
        "    \"format\": \"json\",\n",
        "    \"list\": \"categorymembers\",\n",
        "    \"cmtitle\": category,\n",
        "    \"cmlimit\": \"max\"\n",
        "}\n",
        "\n",
        "response = requests.get(url, params=params)\n",
        "data = json.loads(response.text)\n",
        "\n",
        "if \"query\" in data:\n",
        "\n",
        "    new_articles = [article[\"title\"] for article in data[\"query\"][\"categorymembers\"]]\n",
        "\n",
        "    new_articles = list(set(new_articles) - set(articles))\n",
        "\n",
        "    with open(path + \"society.txt\", \"a\") as f:\n",
        "        for article in new_articles:\n",
        "            f.write(article + \"\\n\")\n",
        "\n",
        "    print(f\"{len(new_articles)} new articles found and saved to society.txt\")\n",
        "else:\n",
        "    new_articles = []\n",
        "    print(\"No articles found in the specified category.\")\n",
        "\n",
        "base_url = \"https://en.wikipedia.org/wiki/\"\n",
        "\n",
        "df = pd.DataFrame(columns=[ \"Title\", \"Paragraphs\"])\n",
        "\n",
        "for article in new_articles:\n",
        "  url = base_url + article\n",
        "  response = requests.get(url)\n",
        "  html = response.content\n",
        "\n",
        "  soup = BeautifulSoup(html,'html.parser')\n",
        "\n",
        "  title = soup.find('h1',{'class':'firstHeading'}).text\n",
        "\n",
        "  paragraphs = []\n",
        "  text_div = soup.find('div', {'class': 'mw-content-container'})\n",
        "  if text_div is not None:\n",
        "    for p in text_div.find_all('p'):\n",
        "      paragraphs.append(p.text)\n",
        "\n",
        "  df.loc[len(df)] = [title, \"\\n\".join(paragraphs)]\n",
        "\n",
        "  if response.status_code != 200:\n",
        "    print(f\"The article '{article}' does not exist on Wikipedia.\")\n",
        "    df = df[df[\"Title\"] != title]\n",
        "\n",
        "df.to_csv('/content/drive/MyDrive/Wiki-datasets/society.csv', mode='a', header=not os.path.isfile('/content/drive/MyDrive/Wiki-datasets/society.csv'), index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thw4EP4vyHtC"
      },
      "source": [
        "###2nd method...Recursion\n",
        "#####Another approach is to use a more powerful server or distributed computing to handle the scraping.But this approch i feel in itself is a limitation so...."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBfx8L6HnD8n"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "path = '/content/drive/MyDrive/Wiki-datasets/'\n",
        "\n",
        "if os.path.isfile(path + 'human_medicine.txt'):\n",
        "    with open(path + 'human_medicine.txt', 'r') as f:\n",
        "        articles = [line.strip() for line in f.readlines()]\n",
        "else:\n",
        "    articles = []\n",
        "\n",
        "category = input(\"Enter the Wikipedia category: \")\n",
        "base_url = \"https://en.wikipedia.org/wiki/\"\n",
        "\n",
        "def scrape_articles_in_category(category):\n",
        "    url = \"https://en.wikipedia.org/w/api.php\"\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"format\": \"json\",\n",
        "        \"list\": \"categorymembers\",\n",
        "        \"cmtitle\": category,\n",
        "        \"cmlimit\": \"max\"\n",
        "    }\n",
        "    response = requests.get(url, params=params)\n",
        "    data = json.loads(response.text)\n",
        "\n",
        "    if \"query\" in data:\n",
        "        new_articles = [article[\"title\"] for article in data[\"query\"][\"categorymembers\"]]\n",
        "        new_articles = list(set(new_articles) - set(articles))\n",
        "        with open(path + \"human_medicine.txt\", \"a\") as f:\n",
        "            for article in new_articles:\n",
        "                f.write(article + \"\\n\")\n",
        "        print(f\"{len(new_articles)} new articles found and saved to human_medicine.txt\")\n",
        "\n",
        "        df = pd.DataFrame(columns=[ \"Title\", \"Paragraphs\"])\n",
        "        for article in new_articles:\n",
        "            url = base_url + article\n",
        "            response = requests.get(url)\n",
        "            html = response.content\n",
        "            soup = BeautifulSoup(html,'html.parser')\n",
        "            title = soup.find('h1',{'class':'firstHeading'}).text\n",
        "            paragraphs = []\n",
        "            text_div = soup.find('div', {'class': 'mw-content-container'})\n",
        "            if text_div is not None:\n",
        "                for p in text_div.find_all('p'):\n",
        "                    paragraphs.append(p.text)\n",
        "            df.loc[len(df)] = [title, \"\\n\".join(paragraphs)]\n",
        "            if response.status_code != 200:\n",
        "                print(f\"The article '{article}' does not exist on Wikipedia.\")\n",
        "                df = df[df[\"Title\"] != title]                             # some error from this line need to be solved.....\n",
        "        df.to_csv('/content/drive/MyDrive/Wiki-datasets/human_medicine.csv', mode='a', header=not os.path.isfile('/content/drive/MyDrive/Wiki-datasets/human_medicine.csv'), index=False)\n",
        "\n",
        "    else:\n",
        "        print(\"No articles found in the specified category.\")\n",
        "\n",
        "    subcategories = []\n",
        "    for article in data[\"query\"][\"categorymembers\"]:\n",
        "        if article[\"ns\"] == 14:\n",
        "            subcategories.append(article[\"title\"])\n",
        "\n",
        "    for subcategory in subcategories:\n",
        "        scrape_articles_in_category(subcategory)\n",
        "\n",
        "scrape_articles_in_category(category)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWjQG5lJnbFS"
      },
      "source": [
        "  \n",
        "\n",
        "###https://en.wikipedia.org/wiki/Wikipedia:Contents\n",
        "#####use this link to directly scrape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbulA-6-c0-O",
        "outputId": "4806ee00-3a2b-46bf-b9a7-747cbecb79bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "                             Title  \\\n",
            "0                   Mahatma Gandhi   \n",
            "1  Assassination of Mahatma Gandhi   \n",
            "2                   Gandhi Jayanti   \n",
            "3      Criticism of Mahatma Gandhi   \n",
            "4         Non-cooperation movement   \n",
            "5             Champaran Satyagraha   \n",
            "6                       Salt March   \n",
            "7              Quit India Movement   \n",
            "8                 Sabarmati Ashram   \n",
            "9                       Satyagraha   \n",
            "\n",
            "                                          Paragraphs  \n",
            "0  \\n\\nMohandas Karamchand Gandhi (/ËˆÉ¡É‘Ëndi, ËˆÉ¡Ã¦n...  \n",
            "1  \\n\\nArrested\\n\\nMahatma Gandhi was assassinate...  \n",
            "2  \\n\\nGandhi Jayanti is an event celebrated in I...  \n",
            "3     Other reasons this message may be displayed:\\n  \n",
            "4  \\n\\nThe Non-cooperation movement was a politic...  \n",
            "5  \\n\\nThe Champaran Satyagraha of 1917 was the f...  \n",
            "6  \\n\\nThe Salt March, also known as the Salt Sat...  \n",
            "7  \\n\\nMahatma Gandhi Indian National Congress\\n\\...  \n",
            "8  \\n\\nSabarmati Ashram (also known as Gandhi Ash...  \n",
            "9  \\n\\nSatyÄgraha (Sanskrit: à¤¸à¤¤à¥à¤¯à¤¾à¤—à¥à¤°à¤¹; satya: \"t...  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "path = '/content/drive/MyDrive/Wiki-datasets/wikipedia_data.csv'\n",
        "\n",
        "df = pd.read_csv(path)\n",
        "\n",
        "print(df.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVz2BS4MgEUu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/Wiki-datasets/wikipedia_data.csv')\n",
        "article_title = 'Ira V. Hiscock'\n",
        "article_df = df.loc[df['Title'] == article_title]\n",
        "\n",
        "print(article_df['Title'].values[0])\n",
        "print(article_df['Paragraphs'].values[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlzsIIrf-iqE"
      },
      "outputs": [],
      "source": [
        "#----> But still see how we will deal with above mathematical ariticle. How we will deal with this?? If we delete this type of data some data will be lost...."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aASEYcmd4TBg"
      },
      "source": [
        "#Paragraph-->Sentence\n",
        "\n",
        "some inspiration:\n",
        "Tokenization for Natural Language Processing\n",
        "\n",
        "Towards Data Science\n",
        "https://towardsdatascience.com/tokenization-for-natural-language-processing-a179a891bad4\n",
        "\n",
        "nltk documentation https://www.nltk.org/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8cLe0f3l04j",
        "outputId": "57fa6e4d-c203-4898-c182-d8b307eae492"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Thiis is an example paragraph.', 'It contains multiple sentences.', 'The sentences will be split into a list.']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import sent_tokenize\n",
        "\n",
        "text = \"Thiis is an example paragraph. It contains multiple sentences. The sentences will be split into a list. \"\n",
        "sentences = sent_tokenize(text)\n",
        "print(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/wikipedia1.csv')\n",
        "\n",
        "df['Sentences'] = df['Paragraphs'].apply(nltk.sent_tokenize)\n",
        "\n",
        "df = df.explode('Sentences')\n",
        "\n",
        "df = df.drop(columns=['Paragraphs'])\n",
        "\n",
        "df.to_csv('/content/drive/MyDrive/wikipedia1.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5HFYP1M8r61",
        "outputId": "dcc3707a-bb1b-410a-d3a3-b070a0da2495"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTCnpd-u64pq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65d5388f-3c36-4ea5-95b4-2d4e9f9693bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Title', 'Sentences'], dtype='object')\n"
          ]
        }
      ],
      "source": [
        "print(df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/Wiki-datasets/culture.csv /content/drive/MyDrive/Wiki-datasets/culture_copy.csv"
      ],
      "metadata": {
        "id": "yH7Fo-UgA_d2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/Wiki-datasets/geography.csv /content/drive/MyDrive/Wiki-datasets/geography_copy.csv"
      ],
      "metadata": {
        "id": "z7hGIBOOdTZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/Wiki-datasets/health.csv /content/drive/MyDrive/Wiki-datasets/health_copy.csv"
      ],
      "metadata": {
        "id": "clk38AC5dolz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/Wiki-datasets/nature.csv /content/drive/MyDrive/Wiki-datasets/nature_copy.csv"
      ],
      "metadata": {
        "id": "-JnBcmwrd4Xy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/Wiki-datasets/people.csv /content/drive/MyDrive/Wiki-datasets/people_copy.csv"
      ],
      "metadata": {
        "id": "LnkrI31IeB4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/Wiki-datasets/philosophy.csv /content/drive/MyDrive/Wiki-datasets/philosophy_copy.csv"
      ],
      "metadata": {
        "id": "rKuEDJzQeMEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/Wiki-datasets/religion.csv /content/drive/MyDrive/Wiki-datasets/religion_copy.csv"
      ],
      "metadata": {
        "id": "mSG1_7vxeR4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/Wiki-datasets/society.csv /content/drive/MyDrive/Wiki-datasets/society_copy.csv"
      ],
      "metadata": {
        "id": "itM8Gaf3eXja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/Wiki-datasets/society_copy.csv')\n",
        "\n",
        "df['Sentences'] = df['Paragraphs'].apply(lambda x: nltk.sent_tokenize(str(x)))\n",
        "\n",
        "pattern = r'\\[\\d+\\]'\n",
        "\n",
        "df['Sentences'] = df['Sentences'].apply(lambda sentence_list: [re.sub(pattern, '', sentence).strip() for sentence in sentence_list])\n",
        "\n",
        "df = df.explode('Sentences')\n",
        "\n",
        "df = df.drop(columns=['Paragraphs'])\n",
        "\n",
        "df.to_csv('/content/drive/MyDrive/Wiki-datasets/society_copy.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0TdaJ_0Cv-CS",
        "outputId": "a610dd45-4d8f-4844-9f3e-333ad9e5c341"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "csv_files = ['/content/drive/MyDrive/Wiki-datasets/culture_copy.csv', '/content/drive/MyDrive/Wiki-datasets/geography_copy.csv', '/content/drive/MyDrive/Wiki-datasets/health_copy.csv','/content/drive/MyDrive/Wiki-datasets/nature_copy.csv','/content/drive/MyDrive/Wiki-datasets/people_copy.csv','/content/drive/MyDrive/Wiki-datasets/philosophy_copy.csv','/content/drive/MyDrive/Wiki-datasets/religion_copy.csv','/content/drive/MyDrive/Wiki-datasets/society_copy.csv']\n",
        "\n",
        "merged_data = pd.DataFrame()\n",
        "\n",
        "for file in csv_files:\n",
        "    data = pd.read_csv(file)\n",
        "\n",
        "    data['File'] = file\n",
        "\n",
        "    merged_data = pd.concat([merged_data, data], ignore_index=True)\n",
        "\n",
        "merged_data.to_csv('/content/drive/MyDrive/Wiki-datasets/merged_data.csv', index=False)\n"
      ],
      "metadata": {
        "id": "7GUJtB3SUIVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Wiki-datasets/merged_data.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "data.head(10)"
      ],
      "metadata": {
        "id": "0r5qNsRMVSqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "csv_files = ['/content/drive/MyDrive/Wiki-datasets/culture_copy.csv', '/content/drive/MyDrive/Wiki-datasets/geography_copy.csv', '/content/drive/MyDrive/Wiki-datasets/health_copy.csv','/content/drive/MyDrive/Wiki-datasets/nature_copy.csv','/content/drive/MyDrive/Wiki-datasets/people_copy.csv','/content/drive/MyDrive/Wiki-datasets/philosophy_copy.csv','/content/drive/MyDrive/Wiki-datasets/religion_copy.csv','/content/drive/MyDrive/Wiki-datasets/society_copy.csv']\n",
        "\n",
        "merged_data = pd.DataFrame()\n",
        "\n",
        "for file in csv_files:\n",
        "    data = pd.read_csv(file)\n",
        "\n",
        "    file_name = os.path.splitext(os.path.basename(file))[0]\n",
        "    file_type = file_name.split('_')[0]\n",
        "    data['File'] = file_type\n",
        "\n",
        "    merged_data = pd.concat([merged_data, data], ignore_index=True)\n",
        "\n",
        "merged_data.to_csv('/content/drive/MyDrive/Wiki-datasets/merged_data1.csv', index=False)\n"
      ],
      "metadata": {
        "id": "K6sxzppLW7dU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Wiki-datasets/merged_data1.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "data.tail(10)"
      ],
      "metadata": {
        "id": "U-U8nZ8eZInD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "middle_data = data.head(4).append(data.tail(7))"
      ],
      "metadata": {
        "id": "iLT1hfivGZFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "middle_data"
      ],
      "metadata": {
        "id": "V8Wtc6KFj4zC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "head_rows = data.head(5).values.tolist()\n",
        "\n",
        "head_rows"
      ],
      "metadata": {
        "id": "azo7Ipg1j8VI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tail_rows = data.tail(10).values.tolist()\n",
        "\n",
        "tail_rows"
      ],
      "metadata": {
        "id": "mQrZNFAbkTwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "bUHeEVaLBiCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv('/content/drive/MyDrive/Wiki-datasets/merged_data1.csv')\n",
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "w2-nm-ocooFE",
        "outputId": "12e60117-5dd3-48c5-87b7-86bd03258ccf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         Title                                          Sentences     File\n",
              "0  V. Kalyanam  V. Kalyanam (15 August 1922 â€“ 4 May 2021) was ...  culture\n",
              "1  V. Kalyanam  He joined the freedom struggle during Quit Ind...  culture\n",
              "2  V. Kalyanam  Kalyanam was just behind Gandhi when Nathuram ...  culture\n",
              "3  V. Kalyanam  According to Kalyanam, Gandhi died instantly a...  culture\n",
              "4  V. Kalyanam  He was the first to inform Nehru and Patel abo...  culture"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0e4ab758-2275-4b5e-8ea1-0256051442d0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Sentences</th>\n",
              "      <th>File</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>V. Kalyanam</td>\n",
              "      <td>V. Kalyanam (15 August 1922 â€“ 4 May 2021) was ...</td>\n",
              "      <td>culture</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>V. Kalyanam</td>\n",
              "      <td>He joined the freedom struggle during Quit Ind...</td>\n",
              "      <td>culture</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>V. Kalyanam</td>\n",
              "      <td>Kalyanam was just behind Gandhi when Nathuram ...</td>\n",
              "      <td>culture</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>V. Kalyanam</td>\n",
              "      <td>According to Kalyanam, Gandhi died instantly a...</td>\n",
              "      <td>culture</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>V. Kalyanam</td>\n",
              "      <td>He was the first to inform Nehru and Patel abo...</td>\n",
              "      <td>culture</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0e4ab758-2275-4b5e-8ea1-0256051442d0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0e4ab758-2275-4b5e-8ea1-0256051442d0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0e4ab758-2275-4b5e-8ea1-0256051442d0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeRc6GU_B8IL",
        "outputId": "626eb2c0-b019-489d-9ee9-2d90fe8fb906"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3373599 entries, 0 to 3373598\n",
            "Data columns (total 3 columns):\n",
            " #   Column     Dtype \n",
            "---  ------     ----- \n",
            " 0   Title      object\n",
            " 1   Sentences  object\n",
            " 2   File       object\n",
            "dtypes: object(3)\n",
            "memory usage: 77.2+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "NEozOWLbCB2E",
        "outputId": "a2da2610-efaa-4ab8-8f52-d316adf5cf9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                    Title  \\\n",
              "count                                             3373599   \n",
              "unique                                              66986   \n",
              "top     List of Supernatural and The Winchesters chara...   \n",
              "freq                                                 2900   \n",
              "\n",
              "                                      Sentences     File  \n",
              "count                                   3348337  3373599  \n",
              "unique                                  3007849        8  \n",
              "top     You can help Wikipedia by expanding it.   health  \n",
              "freq                                      11412   516265  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d42c02f1-cf51-40fb-b601-d9b6d3755da2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Sentences</th>\n",
              "      <th>File</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>3373599</td>\n",
              "      <td>3348337</td>\n",
              "      <td>3373599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>66986</td>\n",
              "      <td>3007849</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>List of Supernatural and The Winchesters chara...</td>\n",
              "      <td>You can help Wikipedia by expanding it.</td>\n",
              "      <td>health</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>2900</td>\n",
              "      <td>11412</td>\n",
              "      <td>516265</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d42c02f1-cf51-40fb-b601-d9b6d3755da2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d42c02f1-cf51-40fb-b601-d9b6d3755da2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d42c02f1-cf51-40fb-b601-d9b6d3755da2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.tokenize import word_tokenize\n",
        "import pickle\n",
        "\n",
        "class Preprocessing:\n",
        "  def __init__(self, num_words, seq_len):\n",
        "    #self.data = '/content/drive/MyDrive/Wiki-datasets/merged_data1.csv'\n",
        "    self.data = '/content/drive/MyDrive/Turing_science_projects_23/Neural language models/Wiki-datasets/merged_data1.csv'\n",
        "    self.num_words = num_words\n",
        "    self.seq_len = seq_len\n",
        "    self.vocabulary = None\n",
        "    self.x_tokenized = None\n",
        "    self.x_padded = None\n",
        "    self.x_raw = None\n",
        "    self.y = None\n",
        "\n",
        "    self.x_train = None\n",
        "    self.x_test = None\n",
        "    self.y_train = None\n",
        "    self.y_test = None\n",
        "\n",
        "  def load_data(self):\n",
        "    # Reads the raw csv file and split into\n",
        "    # sentences (x) and target (y)\n",
        "    df = pd.read_csv(self.data)\n",
        "    #df.drop(['id','keyword','location'], axis=1, inplace=True)\n",
        "    predictors = ['Sentences','Title']\n",
        "    self.x_raw = df[predictors].values\n",
        "    self.y = df['File'].values\n",
        "  '''\n",
        "  def clean_text(self):\n",
        "    # Removes special symbols and just keep\n",
        "    # words in lower or upper form\n",
        "    self.x_raw = [x.lower() for x in self.x_raw]\n",
        "    self.x_raw = [re.sub(r'[^A-Za-z]+', ' ', x) for x in self.x_raw]\n",
        "  '''\n",
        "  def clean_text(self):\n",
        "    # Removes special symbols and just keep\n",
        "    # words in lower or upper form\n",
        "    self.x_raw = [str(x).lower() for x in self.x_raw]\n",
        "    self.x_raw = [re.sub(r'[^A-Za-z]+', ' ', x) for x in self.x_raw]\n",
        "\n",
        "\n",
        "  def text_tokenization(self):\n",
        "    # Tokenizes each sentence by implementing the nltk tool\n",
        "    self.x_raw = [word_tokenize(x) for x in self.x_raw]\n",
        "\n",
        "  def build_vocabulary(self):\n",
        "    # Builds the vocabulary and keeps the \"x\" most frequent word\n",
        "    self.vocabulary = dict()\n",
        "    fdist = nltk.FreqDist()\n",
        "\n",
        "    for sentence in self.x_raw:\n",
        "      for word in sentence:\n",
        "        fdist[word] += 1\n",
        "\n",
        "    common_words = fdist.most_common(self.num_words)\n",
        "\n",
        "    for idx, word in enumerate(common_words):\n",
        "      self.vocabulary[word[0]] = (idx+1)\n",
        "\n",
        "  def word_to_idx(self):\n",
        "    # By using the dictionary (vocabulary), it is transformed\n",
        "    # each token into its index based representatio\n",
        "    self.x_tokenized = list()\n",
        "\n",
        "    for sentence in self.x_raw:\n",
        "      temp_sentence = list()\n",
        "      for word in sentence:\n",
        "        if word in self.vocabulary.keys():\n",
        "          temp_sentence.append(self.vocabulary[word])\n",
        "      self.x_tokenized.append(temp_sentence)\n",
        "\n",
        "  def padding_sentences(self):\n",
        "    # Each sentence which does not fulfill the required le\n",
        "    # it's padded with the index 0\n",
        "    pad_idx = 0\n",
        "    self.x_padded = list()\n",
        "\n",
        "    for sentence in self.x_tokenized:\n",
        "      while len(sentence) < self.seq_len:\n",
        "        sentence.insert(len(sentence), pad_idx)\n",
        "      self.x_padded.append(sentence)\n",
        "\n",
        "    self.x_padded = np.array(self.x_padded)\n",
        "\n",
        "  def split_data(self):\n",
        "    self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(self.x_padded, self.y, test_size=0.2, random_state=42)\n",
        "\n",
        "  def save_data(self, filename):\n",
        "    data = {\n",
        "        'x_train': self.x_train,\n",
        "        'x_test': self.x_test,\n",
        "        'y_train': self.y_train,\n",
        "        'y_test': self.y_test,\n",
        "        'vocabulary': self.vocabulary\n",
        "    }\n",
        "    with open(filename, 'wb') as file:\n",
        "        pickle.dump(data, file)\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "qDN1tShxCFdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAeHhKf3QvZz",
        "outputId": "620076a5-a25c-459e-8d5b-49cc5519b859"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor = Preprocessing(num_words=500000, seq_len=50)\n",
        "preprocessor.load_data()\n",
        "preprocessor.clean_text()\n",
        "preprocessor.text_tokenization()\n",
        "preprocessor.build_vocabulary()\n",
        "preprocessor.word_to_idx()\n",
        "preprocessor.padding_sentences()\n",
        "preprocessor.split_data()\n",
        "#preprocessor.save_data('/content/drive/MyDrive/Wiki-datasets/preprocessing_data.pkl')\n",
        "preprocessor.save_data('/content/drive/MyDrive/Turing_science_projects_23/Neural language models/Wiki-datasets/preprocessing_data5.pkl')\n",
        "''' num_words = 4000: preprocessing_data2.pkl\n",
        "num_words = 50000: preprocessing_data3.pkl\n",
        "num_words = 250000: preprocessing_data4.pkl\n",
        "num_words = 500000: preprocessing_data5.pkl '''"
      ],
      "metadata": {
        "id": "FM0bGsULDMPA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "8deb2fde-3bca-45a8-91a0-c6fd3d781825"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-750ee3e073ec>:89: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  self.x_padded = np.array(self.x_padded)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' num_words = 4000: preprocessing_data2.pkl\\nnum_words = 50000: preprocessing_data3.pkl\\nnum_words = 250000: preprocessing_data4.pkl\\nnum_words = 500000: preprocessing_data5.pkl '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Path to the pickle file\n",
        "#file_path = '/content/drive/MyDrive/Wiki-datasets/preprocessing_data.pkl'\n",
        "file_path = '/content/drive/MyDrive/Turing_science_projects_23/Neural language models/Wiki-datasets/preprocessing_data5.pkl'\n",
        "\n",
        "# Load the data from the pickle file\n",
        "with open(file_path, 'rb') as file:\n",
        "    data = pickle.load(file)\n",
        "\n",
        "# Print the available keys in the data dictionary\n",
        "print(data.keys())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dw3vqekWXXqM",
        "outputId": "d6aeca6b-970e-4c96-ed69-a57a4ba17e62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['x_train', 'x_test', 'y_train', 'y_test', 'vocabulary'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Path to the pickle file\n",
        "#file_path = '/content/drive/MyDrive/Wiki-datasets/preprocessing_data.pkl'\n",
        "file_path = '/content/drive/MyDrive/Turing_science_projects_23/Neural language models/Wiki-datasets/preprocessing_data5.pkl'\n",
        "\n",
        "# Load the data from the pickle file\n",
        "with open(file_path, 'rb') as file:\n",
        "    data = pickle.load(file)\n",
        "\n",
        "# Access a subset of the data\n",
        "\n",
        "subset_data = data['x_test'][:20]\n",
        "\n",
        "# Print the subset of the data\n",
        "for item in subset_data:\n",
        "    print(item)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXg_Hj7qWXdp",
        "outputId": "2bfb766c-0aba-4fa8-c17b-bdd24815e389"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 271, 3, 232, 96, 523, 5, 4284, 6, 442, 2, 150, 4581, 3, 15733, 206, 1628, 123, 4562, 4, 2478, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[2489, 326, 14, 1, 60, 207, 346, 2, 82, 2189, 332, 326, 114, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[15, 857, 6617, 1987, 2, 1, 1824, 492, 2542, 73, 301, 9, 12603, 2, 1, 3473, 11774, 822, 45, 4, 557, 7, 13602, 4, 18, 11788, 11, 1, 1824, 12603, 9927, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[36, 905, 7, 9, 6, 1048, 2, 4521, 1346, 1713, 3951, 3, 19601, 34, 278, 2031, 38, 33, 2481, 45, 5324, 853, 33, 692, 374, 153, 5, 6746, 2275, 14726, 8, 2234, 576, 2052, 374, 3650, 3, 1, 20343, 7359, 886, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[1, 6124, 2532, 5282, 77, 19, 1337, 21252, 5719, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[1, 214990, 74, 6726, 1, 15789, 8, 5, 150, 4061, 3, 1, 15789, 74, 221, 5218, 5, 1, 674, 2, 1, 302, 479, 1, 1510, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[43894, 1097, 6, 676, 577, 388, 6, 84, 110, 25195, 50015, 18, 412, 2041, 109, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[1, 2576, 38747, 13, 236, 4210, 17417, 21662, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[1175, 5554, 11, 194, 2, 756, 5, 820, 5, 2087, 28, 6545, 53, 29, 3005, 4, 29196, 16, 717, 3, 1338, 13, 1, 895, 4765, 200, 1162, 32493, 1175, 7798, 4317, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[11, 1, 886, 14, 113, 2045, 3887, 5444, 6, 119, 151, 180, 17227, 13, 1, 5838, 3887, 3, 1, 128, 2013, 1256, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[19, 71, 104, 8298, 5, 30498, 1, 2255, 3, 104, 21, 1121, 13, 61, 2332, 2711, 2929, 5, 2255, 1489, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[28, 107, 102, 9, 59, 111, 53, 14, 278, 6, 837, 3522, 2, 445, 13, 583, 1734, 4, 849, 231, 38, 5, 6, 3835, 1795, 56, 49, 1, 143, 14, 51, 1092, 1734, 1826, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[1, 10317, 120, 8969, 16677, 1238, 401, 764, 3, 33, 67878, 1411, 604, 675, 19230, 1911, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[45384, 14, 6, 1867, 3, 5054, 9, 929, 1006, 5, 598, 3889, 5, 1, 4722, 99977, 387, 24, 25, 1, 58, 2, 45384, 12, 1043, 137, 26, 21, 6546, 5, 4722, 792, 45384, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[131, 4, 1869, 19, 32, 40, 234, 9, 23, 136282, 7, 87, 1431, 1073, 2, 1101, 17, 1050, 269, 3, 2589, 11, 1, 537, 1101, 3, 1, 226, 30, 4169, 902, 398, 1431, 1101, 602, 33, 100, 8, 33, 197279, 7281, 7, 320, 2132, 269, 3, 2833, 42, 12139, 785, 3, 4779, 2, 1431, 1101, 2615, 2797, 867]\n",
            "[1473, 1, 62018, 960, 1336, 1144, 5, 1126, 373, 1, 62018, 2, 6, 147, 2, 628, 3692, 239, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[136, 1, 240, 1, 60, 5318, 14, 1615, 97, 153, 5, 1, 448, 396, 704, 2054, 54, 4338, 7, 3, 776, 2295, 7, 368, 1204, 3776, 7, 851, 1027, 433, 6796, 7, 38, 7, 7675, 14383, 7, 11528, 4942, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[36496, 12, 3941, 7, 31, 12138, 175, 1702, 10309, 30, 21, 1264, 11, 1, 908, 2, 530, 34195, 9638, 933, 36496, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[75276, 3, 50415, 34, 1, 61, 6069, 2, 55003, 19737, 3, 25210, 9830, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[548, 9274, 5, 1, 2090, 816, 5, 3885, 42, 1199, 11, 1, 671, 2, 1, 5121, 124, 2090, 1168, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Generator function to read the file in chunks\n",
        "def read_data(filename, chunksize):\n",
        "    for chunk in pd.read_csv(filename, chunksize=chunksize):\n",
        "        yield chunk\n",
        "\n",
        "# Define the file path and chunk size\n",
        "filename = \"/content/drive/MyDrive/Wiki-datasets/merged_data1.csv\"\n",
        "chunksize = 100000\n",
        "\n",
        "# Initialize an empty set to store unique words\n",
        "unique_words = set()\n",
        "\n",
        "# Iterate over chunks of the data\n",
        "for data_chunk in read_data(filename, chunksize):\n",
        "    sentences = data_chunk[\"Sentences\"]\n",
        "    words = \" \".join(str(sentence) for sentence in sentences).split()\n",
        "    unique_words = set(words)\n",
        "\n",
        "\n",
        "# Count the number of unique words\n",
        "num_unique_words = len(unique_words)\n",
        "\n",
        "print(\"Number of different words in the corpus:\", num_unique_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_NbCq1KpEy-",
        "outputId": "8136d5e6-7ee0-43e4-d91e-98e95111e39f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of different words in the corpus: 133248\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Number of different words in the corpus: 57919   chunk= 50000\n",
        "\n",
        "---\n",
        "Number of different words in the corpus: 93876   chunk= 90000\n",
        "\n",
        "---\n",
        "Number of different words in the corpus: 133248   chunk = 100000\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PAidCQ0tv9yQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the data into a DataFrame\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/Wiki-datasets/merged_data1.csv\")\n",
        "\n",
        "# Access the \"Sentences\" column\n",
        "sentences = data[\"Sentences\"].astype(str)\n",
        "\n",
        "# Split each sentence into words\n",
        "all_words = \" \".join(sentences).split()\n",
        "\n",
        "# Count the number of unique words\n",
        "num_unique_words = len(set(all_words))\n",
        "\n",
        "print(\"Number of different words in the corpus:\", num_unique_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AoZMygUtuebX",
        "outputId": "ba669e60-a913-48a7-81dd-8eb10695ed27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of different words in the corpus: 2058601\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jBKe48t5wk3v"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}